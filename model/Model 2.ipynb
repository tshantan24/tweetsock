{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation, Embedding, CuDNNLSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Handle</th>\n",
       "      <th>Party</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Gov. @ricardorossello's comments degrading wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Looks like Trump will end his discriminatory c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>For several years we sought to replace our sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Proud to announce that @fema awarded @PolkCoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>STURDY Bill passed @EnergyCommerce Cmte today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Enough is enough! Billionaire super predator J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>We continue our efforts to provide American Ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>We're committed to defending quality &amp;amp; aff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>Robocalls aren’t just annoying. Many are outri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>Democrat</td>\n",
       "      <td>This is why we continue to fight @jediabetical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Handle     Party                                              Tweet\n",
       "0  RepDarrenSoto  Democrat  Gov. @ricardorossello's comments degrading wom...\n",
       "1  RepDarrenSoto  Democrat  Looks like Trump will end his discriminatory c...\n",
       "2  RepDarrenSoto  Democrat  For several years we sought to replace our sta...\n",
       "3  RepDarrenSoto  Democrat  Proud to announce that @fema awarded @PolkCoun...\n",
       "4  RepDarrenSoto  Democrat  STURDY Bill passed @EnergyCommerce Cmte today ...\n",
       "5  RepDarrenSoto  Democrat  Enough is enough! Billionaire super predator J...\n",
       "6  RepDarrenSoto  Democrat  We continue our efforts to provide American Ci...\n",
       "7  RepDarrenSoto  Democrat  We're committed to defending quality &amp; aff...\n",
       "8  RepDarrenSoto  Democrat  Robocalls aren’t just annoying. Many are outri...\n",
       "9  RepDarrenSoto  Democrat  This is why we continue to fight @jediabetical..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/data.csv')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Handle</th>\n",
       "      <th>Party</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>0</td>\n",
       "      <td>Gov. @ricardorossello's comments degrading wom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>0</td>\n",
       "      <td>Looks like Trump will end his discriminatory c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>0</td>\n",
       "      <td>For several years we sought to replace our sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>0</td>\n",
       "      <td>Proud to announce that @fema awarded @PolkCoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RepDarrenSoto</td>\n",
       "      <td>0</td>\n",
       "      <td>STURDY Bill passed @EnergyCommerce Cmte today ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Handle  Party                                              Tweet\n",
       "0  RepDarrenSoto      0  Gov. @ricardorossello's comments degrading wom...\n",
       "1  RepDarrenSoto      0  Looks like Trump will end his discriminatory c...\n",
       "2  RepDarrenSoto      0  For several years we sought to replace our sta...\n",
       "3  RepDarrenSoto      0  Proud to announce that @fema awarded @PolkCoun...\n",
       "4  RepDarrenSoto      0  STURDY Bill passed @EnergyCommerce Cmte today ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Party'] = pd.Categorical(df.Party)\n",
    "df['Party'] = pd.get_dummies(df['Party'], drop_first=True)\n",
    "# df[df['Party'] == 0]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>0</b> - Democrat <br>\n",
    "<b>1</b> - Republican"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000,)\n",
      "(90000,)\n"
     ]
    }
   ],
   "source": [
    "X = df['Tweet']\n",
    "Y = df['Party']\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gov. @ricardorossello's comments degrading women, including my dear friend @MMViverito, are unacceptable. I condemn these demeaning words. Now more than ever, Puerto Rico is in need of strong leadership. I urge the Governor to use appropriate language &amp; always respect women.\n"
     ]
    }
   ],
   "source": [
    "print(X.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "punctuation = punctuation + \"—\\n\\t\"\n",
    "regex = re.compile('[%s]' % re.escape(punctuation))\n",
    "\n",
    "def remove_punctuations(sentence):\n",
    "    return regex.sub('', sentence)\n",
    "\n",
    "def remove_link(sentence):\n",
    "    sentence = str(sentence)\n",
    "    return sentence[:sentence.find(\"https://\")]\n",
    "\n",
    "def pre_processing(X, **kwargs):\n",
    "    # Replaces special characters\n",
    "    X = X.str.replace('&amp;', \"and\")\n",
    "    X = X.str.replace('\\xa0', \" \")\n",
    "    X = X.str.replace('\\u2003', \" \")\n",
    "    \n",
    "    #Removes links\n",
    "    X = X.apply(remove_link)\n",
    "    \n",
    "    #Removes punctuations and converts into lowercase\n",
    "    X = X.apply(remove_punctuations)\n",
    "    X = X.apply(str.lower)\n",
    "    \n",
    "    #Removes null values\n",
    "    ind = list(X[X==\"\"].index)\n",
    "    x = X.drop(ind)\n",
    "    \n",
    "    if 'Y' in kwargs:\n",
    "        y = kwargs['Y'].drop(ind)\n",
    "        return x, y\n",
    "        \n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90000,)\n",
      "(90000,)\n"
     ]
    }
   ],
   "source": [
    "x, y = pre_processing(X, Y=Y)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        gov ricardorossellos comments degrading women ...\n",
       "1        looks like trump will end his discriminatory c...\n",
       "2        for several years we sought to replace our sta...\n",
       "3        proud to announce that fema awarded polkcounty...\n",
       "4        sturdy bill passed energycommerce cmte today w...\n",
       "5        enough is enough billionaire super predator je...\n",
       "6        we continue our efforts to provide american ci...\n",
       "7        were committed to defending quality and afford...\n",
       "8        robocalls aren’t just annoying many are outrig...\n",
       "9        this is why we continue to fight jediabetical ...\n",
       "10       trio of fla congressional members wrap up over...\n",
       "11       freedom isn’t free it’s protected by the coura...\n",
       "12       major victory days after ussupremecourt decide...\n",
       "13       passed ✅ proud to join my colleagues from flor...\n",
       "14       success housedemocrats passed my proposal to a...\n",
       "15       2 yrs after hurricane maria and one yr after w...\n",
       "16       our congressional blockchain caucus is meeting...\n",
       "17       have a question about the latest tech legislat...\n",
       "18       victory illintended citizenship question is bl...\n",
       "19       disappointed that us supreme court declined to...\n",
       "20       we passed key bills to prevent illegal robocal...\n",
       "21       did you know we passed 56 legislative items in...\n",
       "22       we passed 600m to fund nutrition assistance pr...\n",
       "23       forthepeople housedemocrats 🆚 donothingsenate ...\n",
       "24       jediabetic1 i agree this is a travesty we expe...\n",
       "25       we housedemocrats are pushing a supplement fun...\n",
       "26       honored to rededicate the statue of tuskegee a...\n",
       "27       the cost of climate change to florida is 76 bi...\n",
       "28       so many beautiful princesses at ⁦librarycongre...\n",
       "29       trish536 cleanairmoms lincoln was missed and h...\n",
       "                               ...                        \n",
       "89970    prescription drugs are too expensive plain and...\n",
       "89971    in case you missed it i worked with corporal r...\n",
       "89972    thanks to sjsheriff for taking the time to mee...\n",
       "89973    i’ve heard from veterans across the central va...\n",
       "89974    glad to meet with so many of the brave men and...\n",
       "89975    i was honored to work with corporal ronil sing...\n",
       "89976    corporal ronil singh is an american hero i sat...\n",
       "89977    during national police week i want to take a m...\n",
       "89978    tonight i’ll stand with the families of corpor...\n",
       "89979    in case you missed it last week my amendment t...\n",
       "89980    did you know that my office can help you navig...\n",
       "89981    in case you missed it i sat down with fox40 to...\n",
       "89982    glad to be able to help master sergeant hurche...\n",
       "89983    we need more mental health coverage in the val...\n",
       "89984    proud to have worked closely with darrell cord...\n",
       "89985    i came to washington to work for the valley no...\n",
       "89986    the altamont pass is a mess and long commutes ...\n",
       "89987    watch i sat down with fox40s joekhaliltv to ta...\n",
       "89988    today i passed an amendment to ensure that peo...\n",
       "89989    i came to congress to protect people with pree...\n",
       "89990    congrats to kaylee harvey of tracy for winning...\n",
       "89991    watch my full conversation yesterday with nikk...\n",
       "89992    always great to see central valley folks in wa...\n",
       "89993    what do ted cruz elizabeth warren and i have i...\n",
       "89994    today i sat down with nikkilaurenzo to chat th...\n",
       "89995    this week is nationalnursesweek and i want to ...\n",
       "89996    our students deserve every opportunity to lear...\n",
       "89997                                   just doing my job \n",
       "89998    working hard to set a new standard of accessib...\n",
       "89999    got to sit down with joekhaliltv to talk about...\n",
       "Name: Tweet, Length: 89541, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example\n",
    "d = pre_processing(X)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "<b><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=420)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tokenizer and creating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(oov_token=\"UNK\")\n",
    "a = list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of a: 89541\n"
     ]
    }
   ],
   "source": [
    "print(\"Len of a: {}\".format(len(a)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 89911\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "print(\"Vocabulary size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 74\n"
     ]
    }
   ],
   "source": [
    "max_sent_len = len(max(x, key=len).split()) + 1\n",
    "print(\"Maximum sentence length: {}\".format(max_sent_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentences(sentence, word_to_index):\n",
    "    encoded = np.zeros((1, max_sent_len))\n",
    "    sentence_words = sentence.lower().split()\n",
    "    j = 0\n",
    "    for w in sentence_words:\n",
    "        encoded[0, j] = word_to_index[w]\n",
    "        j += 1\n",
    "    \n",
    "    return np.float32(encoded)\n",
    "\n",
    "def encode_and_pad(X):\n",
    "    encoded_x = t.texts_to_sequences(X)\n",
    "    padded_x = pad_sequences(encoded_x, maxlen=max_sent_len, padding='post')\n",
    "    return padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  99,   93, 1529, ...,    0,    0,    0],\n",
       "       [   2,  839,   10, ...,    0,    0,    0],\n",
       "       [1126, 1192, 5953, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,   41,  271, ...,    0,    0,    0],\n",
       "       [  26,   10, 2651, ...,    0,    0,    0],\n",
       "       [ 128,   13,  590, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train = encode_and_pad(X_train)\n",
    "padded_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71632, 74)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0719 00:23:53.289809 13600 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_sent_len, trainable=False)\n",
    "trained_embedding_layer = Embedding(input_dim=vocab_size, output_dim=emb_dim, input_length=max_sent_len, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# encoded_X_train1 = tf.convert_to_tensor(encoded_X_train, np.float32)\n",
    "layer_1 = embedding_layer(padded_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_layer_1 = trained_embedding_layer(padded_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "learning_rate = 0.05\n",
    "epochs = 50\n",
    "num_hidden_units = 128\n",
    "timesteps = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0719 00:26:17.933421 13600 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0719 00:26:17.937412 13600 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0719 00:26:17.938408 13600 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0719 00:26:17.940403 13600 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    embedding_layer,\n",
    "    Bidirectional(CuDNNLSTM(300, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(padded_X_train, y_train, epochs=200, batch_size = 400, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 74, 50)            4495550   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 600)               844800    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1202      \n",
      "=================================================================\n",
      "Total params: 5,341,552\n",
      "Trainable params: 846,002\n",
      "Non-trainable params: 4,495,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0718 00:53:54.259601 20456 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0718 00:53:54.263591 20456 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0718 00:53:54.264588 20456 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0718 00:53:54.265586 20456 deprecation.py:506] From C:\\Users\\tshan\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential([\n",
    "    embedding_layer,\n",
    "    Bidirectional(CuDNNLSTM(128, return_sequences=True)),\n",
    "    Dropout(0.5),\n",
    "    CuDNNLSTM(128),\n",
    "    Dropout(0.5),\n",
    "    Dense(2),\n",
    "    Activation('softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 74, 50)            4495550   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 74, 256)           184320    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 74, 256)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     (None, 128)               197632    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 4,877,760\n",
      "Trainable params: 382,210\n",
      "Non-trainable params: 4,495,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "71632/71632 [==============================] - 29s 398us/sample - loss: 0.6933 - acc: 0.4984\n",
      "Epoch 2/50\n",
      "71632/71632 [==============================] - 28s 398us/sample - loss: 0.6932 - acc: 0.5016\n",
      "Epoch 3/50\n",
      "71632/71632 [==============================] - 35s 489us/sample - loss: 0.6932 - acc: 0.5009\n",
      "Epoch 4/50\n",
      "71632/71632 [==============================] - 35s 483us/sample - loss: 0.6931 - acc: 0.5029\n",
      "Epoch 5/50\n",
      "71632/71632 [==============================] - 31s 435us/sample - loss: 0.6932 - acc: 0.4982\n",
      "Epoch 6/50\n",
      "71632/71632 [==============================] - 32s 446us/sample - loss: 0.6932 - acc: 0.4979\n",
      "Epoch 7/50\n",
      "71632/71632 [==============================] - 39s 545us/sample - loss: 0.6932 - acc: 0.5007\n",
      "Epoch 8/50\n",
      "71632/71632 [==============================] - 33s 466us/sample - loss: 0.6932 - acc: 0.4970\n",
      "Epoch 9/50\n",
      "71632/71632 [==============================] - 34s 475us/sample - loss: 0.6932 - acc: 0.5004\n",
      "Epoch 10/50\n",
      "71632/71632 [==============================] - 35s 492us/sample - loss: 0.6932 - acc: 0.5016 - loss: 0.6932 - acc: 0.50\n",
      "Epoch 11/50\n",
      "71632/71632 [==============================] - 34s 468us/sample - loss: 0.6932 - acc: 0.4999\n",
      "Epoch 12/50\n",
      "71632/71632 [==============================] - 42s 590us/sample - loss: 0.6932 - acc: 0.4979\n",
      "Epoch 13/50\n",
      "71632/71632 [==============================] - 37s 520us/sample - loss: 0.6932 - acc: 0.4985\n",
      "Epoch 14/50\n",
      "71632/71632 [==============================] - 38s 524us/sample - loss: 0.6931 - acc: 0.5013\n",
      "Epoch 15/50\n",
      "71632/71632 [==============================] - 38s 530us/sample - loss: 0.6932 - acc: 0.4999\n",
      "Epoch 16/50\n",
      "71632/71632 [==============================] - 37s 512us/sample - loss: 0.6932 - acc: 0.5009\n",
      "Epoch 17/50\n",
      "71632/71632 [==============================] - 49s 687us/sample - loss: 0.6932 - acc: 0.4997\n",
      "Epoch 18/50\n",
      "71632/71632 [==============================] - 39s 546us/sample - loss: 0.6932 - acc: 0.4999\n",
      "Epoch 19/50\n",
      "71632/71632 [==============================] - 18s 256us/sample - loss: 0.6931 - acc: 0.4996\n",
      "Epoch 20/50\n",
      "71632/71632 [==============================] - 16s 225us/sample - loss: 0.6932 - acc: 0.4993\n",
      "Epoch 21/50\n",
      "71632/71632 [==============================] - 19s 260us/sample - loss: 0.6931 - acc: 0.5010\n",
      "Epoch 22/50\n",
      "71632/71632 [==============================] - 18s 251us/sample - loss: 0.6932 - acc: 0.4997\n",
      "Epoch 23/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.5008\n",
      "Epoch 24/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5011\n",
      "Epoch 25/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5003\n",
      "Epoch 26/50\n",
      "71632/71632 [==============================] - 18s 247us/sample - loss: 0.6931 - acc: 0.5011\n",
      "Epoch 27/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.4961\n",
      "Epoch 28/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.4998\n",
      "Epoch 29/50\n",
      "71632/71632 [==============================] - 17s 244us/sample - loss: 0.6932 - acc: 0.4994\n",
      "Epoch 30/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5000\n",
      "Epoch 31/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.4984\n",
      "Epoch 32/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6931 - acc: 0.5033\n",
      "Epoch 33/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5024\n",
      "Epoch 34/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5007\n",
      "Epoch 35/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.4979\n",
      "Epoch 36/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5012\n",
      "Epoch 37/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.5002 - loss: 0.6932 -\n",
      "Epoch 38/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5002\n",
      "Epoch 39/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.4988\n",
      "Epoch 40/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5021\n",
      "Epoch 41/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.4981\n",
      "Epoch 42/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.4985\n",
      "Epoch 43/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5005\n",
      "Epoch 44/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5005\n",
      "Epoch 45/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.4987 - loss: 0.6932 - acc: 0.49\n",
      "Epoch 46/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.4976\n",
      "Epoch 47/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.4993\n",
      "Epoch 48/50\n",
      "71632/71632 [==============================] - 18s 245us/sample - loss: 0.6932 - acc: 0.5031\n",
      "Epoch 49/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.4986\n",
      "Epoch 50/50\n",
      "71632/71632 [==============================] - 18s 246us/sample - loss: 0.6932 - acc: 0.5004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a9009e8cc0>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(padded_X_train, y_train, epochs=50, batch_size = 400, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "71632/71632 [==============================] - 11s 160us/sample - loss: 0.6892 - acc: 0.5384\n",
      "Epoch 2/150\n",
      "71632/71632 [==============================] - 11s 155us/sample - loss: 0.6859 - acc: 0.5510 - loss: 0.6860 - acc:\n",
      "Epoch 3/150\n",
      "71632/71632 [==============================] - 11s 155us/sample - loss: 0.6825 - acc: 0.5606\n",
      "Epoch 4/150\n",
      "71632/71632 [==============================] - 11s 156us/sample - loss: 0.6812 - acc: 0.5657\n",
      "Epoch 5/150\n",
      "71632/71632 [==============================] - 11s 157us/sample - loss: 0.6798 - acc: 0.5664 - loss: 0.6799 - acc:\n",
      "Epoch 6/150\n",
      "71632/71632 [==============================] - 11s 157us/sample - loss: 0.6783 - acc: 0.5686 - loss: 0.6782 - acc\n",
      "Epoch 7/150\n",
      "71632/71632 [==============================] - 11s 158us/sample - loss: 0.6776 - acc: 0.5690\n",
      "Epoch 8/150\n",
      "71632/71632 [==============================] - 11s 158us/sample - loss: 0.6807 - acc: 0.5608\n",
      "Epoch 9/150\n",
      "71632/71632 [==============================] - 11s 159us/sample - loss: 0.6776 - acc: 0.5704\n",
      "Epoch 10/150\n",
      "71632/71632 [==============================] - 11s 159us/sample - loss: 0.6761 - acc: 0.5746\n",
      "Epoch 11/150\n",
      "71632/71632 [==============================] - 11s 159us/sample - loss: 0.6758 - acc: 0.5715\n",
      "Epoch 12/150\n",
      "71632/71632 [==============================] - 11s 159us/sample - loss: 0.6752 - acc: 0.5753\n",
      "Epoch 13/150\n",
      "71632/71632 [==============================] - 12s 168us/sample - loss: 0.6744 - acc: 0.5749\n",
      "Epoch 14/150\n",
      "71632/71632 [==============================] - 12s 172us/sample - loss: 0.6740 - acc: 0.5775\n",
      "Epoch 15/150\n",
      "71632/71632 [==============================] - 12s 171us/sample - loss: 0.6736 - acc: 0.5773\n",
      "Epoch 16/150\n",
      "71632/71632 [==============================] - 12s 171us/sample - loss: 0.6735 - acc: 0.5774\n",
      "Epoch 17/150\n",
      "71632/71632 [==============================] - 13s 181us/sample - loss: 0.6727 - acc: 0.5786\n",
      "Epoch 18/150\n",
      "71632/71632 [==============================] - 13s 182us/sample - loss: 0.6725 - acc: 0.5782\n",
      "Epoch 19/150\n",
      "71632/71632 [==============================] - 15s 209us/sample - loss: 0.6721 - acc: 0.5803\n",
      "Epoch 20/150\n",
      "71632/71632 [==============================] - 15s 208us/sample - loss: 0.6711 - acc: 0.5806\n",
      "Epoch 21/150\n",
      "71632/71632 [==============================] - 15s 208us/sample - loss: 0.6703 - acc: 0.5826\n",
      "Epoch 22/150\n",
      "71632/71632 [==============================] - 15s 208us/sample - loss: 0.6703 - acc: 0.5835\n",
      "Epoch 23/150\n",
      "71632/71632 [==============================] - 15s 209us/sample - loss: 0.6691 - acc: 0.5828\n",
      "Epoch 24/150\n",
      "71632/71632 [==============================] - 15s 209us/sample - loss: 0.6678 - acc: 0.5858\n",
      "Epoch 25/150\n",
      "71632/71632 [==============================] - 15s 209us/sample - loss: 0.6664 - acc: 0.5882\n",
      "Epoch 26/150\n",
      "71632/71632 [==============================] - 20s 273us/sample - loss: 0.6640 - acc: 0.5924\n",
      "Epoch 27/150\n",
      "71632/71632 [==============================] - 15s 213us/sample - loss: 0.6620 - acc: 0.5931\n",
      "Epoch 28/150\n",
      "71632/71632 [==============================] - 17s 240us/sample - loss: 0.6583 - acc: 0.5981\n",
      "Epoch 29/150\n",
      "71632/71632 [==============================] - 16s 230us/sample - loss: 0.6579 - acc: 0.5980 - loss: 0.6577 - acc: 0\n",
      "Epoch 30/150\n",
      "71632/71632 [==============================] - 17s 231us/sample - loss: 0.6563 - acc: 0.6005\n",
      "Epoch 31/150\n",
      "71632/71632 [==============================] - 17s 231us/sample - loss: 0.6539 - acc: 0.6047\n",
      "Epoch 32/150\n",
      "71632/71632 [==============================] - 18s 247us/sample - loss: 0.6514 - acc: 0.6084\n",
      "Epoch 33/150\n",
      "71632/71632 [==============================] - 18s 250us/sample - loss: 0.6491 - acc: 0.6123\n",
      "Epoch 34/150\n",
      "71632/71632 [==============================] - 18s 250us/sample - loss: 0.6473 - acc: 0.6128\n",
      "Epoch 35/150\n",
      "71632/71632 [==============================] - 18s 250us/sample - loss: 0.6490 - acc: 0.6112\n",
      "Epoch 36/150\n",
      "71632/71632 [==============================] - 18s 250us/sample - loss: 0.6432 - acc: 0.6187\n",
      "Epoch 37/150\n",
      "71632/71632 [==============================] - 18s 250us/sample - loss: 0.6403 - acc: 0.6219\n",
      "Epoch 38/150\n",
      "71632/71632 [==============================] - 18s 250us/sample - loss: 0.6619 - acc: 0.6082\n",
      "Epoch 39/150\n",
      "71632/71632 [==============================] - 18s 250us/sample - loss: 0.6416 - acc: 0.6222\n",
      "Epoch 40/150\n",
      "71632/71632 [==============================] - 18s 251us/sample - loss: 0.6372 - acc: 0.6249\n",
      "Epoch 41/150\n",
      "71632/71632 [==============================] - 22s 301us/sample - loss: 0.6356 - acc: 0.6282\n",
      "Epoch 42/150\n",
      "71632/71632 [==============================] - 21s 300us/sample - loss: 0.6327 - acc: 0.6299\n",
      "Epoch 43/150\n",
      "71632/71632 [==============================] - 19s 271us/sample - loss: 0.6312 - acc: 0.6335\n",
      "Epoch 44/150\n",
      "71632/71632 [==============================] - 21s 294us/sample - loss: 0.6279 - acc: 0.6355\n",
      "Epoch 45/150\n",
      "71632/71632 [==============================] - 20s 284us/sample - loss: 0.6262 - acc: 0.6398\n",
      "Epoch 46/150\n",
      "71632/71632 [==============================] - 20s 285us/sample - loss: 0.6236 - acc: 0.6400\n",
      "Epoch 47/150\n",
      "71632/71632 [==============================] - 20s 283us/sample - loss: 0.6210 - acc: 0.6432\n",
      "Epoch 48/150\n",
      "71632/71632 [==============================] - 20s 284us/sample - loss: 0.6217 - acc: 0.6432\n",
      "Epoch 49/150\n",
      "71632/71632 [==============================] - 21s 287us/sample - loss: 0.6208 - acc: 0.6433\n",
      "Epoch 50/150\n",
      "71632/71632 [==============================] - 25s 354us/sample - loss: 0.6158 - acc: 0.6471\n",
      "Epoch 51/150\n",
      "71632/71632 [==============================] - 23s 325us/sample - loss: 0.6123 - acc: 0.6534\n",
      "Epoch 52/150\n",
      "71632/71632 [==============================] - 23s 316us/sample - loss: 0.6092 - acc: 0.6529\n",
      "Epoch 53/150\n",
      "71632/71632 [==============================] - 21s 299us/sample - loss: 0.6066 - acc: 0.6593\n",
      "Epoch 54/150\n",
      "71632/71632 [==============================] - 22s 300us/sample - loss: 0.6027 - acc: 0.6603\n",
      "Epoch 55/150\n",
      "71632/71632 [==============================] - 21s 300us/sample - loss: 0.5990 - acc: 0.6636\n",
      "Epoch 56/150\n",
      "71632/71632 [==============================] - 21s 300us/sample - loss: 0.5971 - acc: 0.6652\n",
      "Epoch 57/150\n",
      "71632/71632 [==============================] - 22s 313us/sample - loss: 0.5933 - acc: 0.6686\n",
      "Epoch 58/150\n",
      "71632/71632 [==============================] - 22s 312us/sample - loss: 0.5885 - acc: 0.6743\n",
      "Epoch 59/150\n",
      "71632/71632 [==============================] - 20s 274us/sample - loss: 0.5853 - acc: 0.6756\n",
      "Epoch 60/150\n",
      "71632/71632 [==============================] - 22s 309us/sample - loss: 0.5794 - acc: 0.6797\n",
      "Epoch 61/150\n",
      "71632/71632 [==============================] - 22s 301us/sample - loss: 0.5773 - acc: 0.6816\n",
      "Epoch 62/150\n",
      "71632/71632 [==============================] - 22s 302us/sample - loss: 0.5718 - acc: 0.6862\n",
      "Epoch 63/150\n",
      "71632/71632 [==============================] - 18s 251us/sample - loss: 0.5650 - acc: 0.6924\n",
      "Epoch 64/150\n",
      "71632/71632 [==============================] - 21s 297us/sample - loss: 0.5580 - acc: 0.6982\n",
      "Epoch 65/150\n",
      "71632/71632 [==============================] - 21s 300us/sample - loss: 0.5551 - acc: 0.7001\n",
      "Epoch 66/150\n",
      "71632/71632 [==============================] - 33s 468us/sample - loss: 0.5482 - acc: 0.7058\n",
      "Epoch 67/150\n",
      "71632/71632 [==============================] - 54s 754us/sample - loss: 0.5437 - acc: 0.7083\n",
      "Epoch 68/150\n",
      "71632/71632 [==============================] - 47s 659us/sample - loss: 0.5359 - acc: 0.7164\n",
      "Epoch 69/150\n",
      "71632/71632 [==============================] - 51s 709us/sample - loss: 0.5287 - acc: 0.7198\n",
      "Epoch 70/150\n",
      "71632/71632 [==============================] - 42s 590us/sample - loss: 0.5379 - acc: 0.7153\n",
      "Epoch 71/150\n",
      "71632/71632 [==============================] - 42s 583us/sample - loss: 0.5230 - acc: 0.7249\n",
      "Epoch 72/150\n",
      "71632/71632 [==============================] - 43s 595us/sample - loss: 0.5129 - acc: 0.7311\n",
      "Epoch 73/150\n",
      "71632/71632 [==============================] - 42s 580us/sample - loss: 0.5046 - acc: 0.7368\n",
      "Epoch 74/150\n",
      "71632/71632 [==============================] - 51s 710us/sample - loss: 0.4987 - acc: 0.7421\n",
      "Epoch 75/150\n",
      "71632/71632 [==============================] - 46s 639us/sample - loss: 0.4943 - acc: 0.7447\n",
      "Epoch 76/150\n",
      "71632/71632 [==============================] - 44s 616us/sample - loss: 0.4842 - acc: 0.7523\n",
      "Epoch 77/150\n",
      "71632/71632 [==============================] - 43s 602us/sample - loss: 0.4772 - acc: 0.7565\n",
      "Epoch 78/150\n",
      "71632/71632 [==============================] - 44s 608us/sample - loss: 0.4684 - acc: 0.7633\n",
      "Epoch 79/150\n",
      "71632/71632 [==============================] - 43s 597us/sample - loss: 0.4634 - acc: 0.7650\n",
      "Epoch 80/150\n",
      "71632/71632 [==============================] - 44s 611us/sample - loss: 0.4771 - acc: 0.7571\n",
      "Epoch 81/150\n",
      "71632/71632 [==============================] - 44s 609us/sample - loss: 0.4464 - acc: 0.7772\n",
      "Epoch 82/150\n",
      "71632/71632 [==============================] - 44s 609us/sample - loss: 0.4410 - acc: 0.7814\n",
      "Epoch 83/150\n",
      "71632/71632 [==============================] - 39s 551us/sample - loss: 0.4324 - acc: 0.7852\n",
      "Epoch 84/150\n",
      "71632/71632 [==============================] - 44s 614us/sample - loss: 0.4275 - acc: 0.7886\n",
      "Epoch 85/150\n",
      "71632/71632 [==============================] - 44s 607us/sample - loss: 0.4159 - acc: 0.7959\n",
      "Epoch 86/150\n",
      "71632/71632 [==============================] - 43s 605us/sample - loss: 0.4080 - acc: 0.8015\n",
      "Epoch 87/150\n",
      "71632/71632 [==============================] - 44s 618us/sample - loss: 0.4018 - acc: 0.8045\n",
      "Epoch 88/150\n",
      "71632/71632 [==============================] - 43s 595us/sample - loss: 0.3941 - acc: 0.8090\n",
      "Epoch 89/150\n",
      "71632/71632 [==============================] - 43s 607us/sample - loss: 0.3919 - acc: 0.8117\n",
      "Epoch 90/150\n",
      "71632/71632 [==============================] - 55s 766us/sample - loss: 0.3805 - acc: 0.8181\n",
      "Epoch 91/150\n",
      "71632/71632 [==============================] - 38s 536us/sample - loss: 0.3712 - acc: 0.8227\n",
      "Epoch 92/150\n",
      "71632/71632 [==============================] - 42s 588us/sample - loss: 0.3564 - acc: 0.8314\n",
      "Epoch 93/150\n",
      "71632/71632 [==============================] - 51s 707us/sample - loss: 0.3503 - acc: 0.8340\n",
      "Epoch 94/150\n",
      "71632/71632 [==============================] - 44s 612us/sample - loss: 0.3428 - acc: 0.8386\n",
      "Epoch 95/150\n",
      "71632/71632 [==============================] - 49s 686us/sample - loss: 0.3389 - acc: 0.8409\n",
      "Epoch 96/150\n",
      "71632/71632 [==============================] - 43s 604us/sample - loss: 0.3281 - acc: 0.8472\n",
      "Epoch 97/150\n",
      "71632/71632 [==============================] - 52s 730us/sample - loss: 0.3189 - acc: 0.8516\n",
      "Epoch 98/150\n",
      "71632/71632 [==============================] - 39s 538us/sample - loss: 0.3138 - acc: 0.8551\n",
      "Epoch 99/150\n",
      "71632/71632 [==============================] - 54s 756us/sample - loss: 0.3183 - acc: 0.8546\n",
      "Epoch 100/150\n",
      "71632/71632 [==============================] - 47s 649us/sample - loss: 0.3083 - acc: 0.8574\n",
      "Epoch 101/150\n",
      "71632/71632 [==============================] - 51s 717us/sample - loss: 0.2899 - acc: 0.8672\n",
      "Epoch 102/150\n",
      "71632/71632 [==============================] - 39s 551us/sample - loss: 0.2839 - acc: 0.8695\n",
      "Epoch 103/150\n",
      "71632/71632 [==============================] - 50s 703us/sample - loss: 0.2880 - acc: 0.8698\n",
      "Epoch 104/150\n",
      "71632/71632 [==============================] - 48s 671us/sample - loss: 0.2755 - acc: 0.8754\n",
      "Epoch 105/150\n",
      "71632/71632 [==============================] - 53s 738us/sample - loss: 0.2741 - acc: 0.8766\n",
      "Epoch 106/150\n",
      "71632/71632 [==============================] - 46s 643us/sample - loss: 0.2612 - acc: 0.8840\n",
      "Epoch 107/150\n",
      "71632/71632 [==============================] - 44s 608us/sample - loss: 0.2560 - acc: 0.8851\n",
      "Epoch 108/150\n",
      "71632/71632 [==============================] - 43s 595us/sample - loss: 0.2507 - acc: 0.8872\n",
      "Epoch 109/150\n",
      "71632/71632 [==============================] - 43s 601us/sample - loss: 0.2434 - acc: 0.8904\n",
      "Epoch 110/150\n",
      "71632/71632 [==============================] - 49s 688us/sample - loss: 0.2476 - acc: 0.8887\n",
      "Epoch 111/150\n",
      "71632/71632 [==============================] - 52s 730us/sample - loss: 0.2396 - acc: 0.8943\n",
      "Epoch 112/150\n",
      "71632/71632 [==============================] - 45s 622us/sample - loss: 0.2377 - acc: 0.8952\n",
      "Epoch 113/150\n",
      "71632/71632 [==============================] - 51s 707us/sample - loss: 0.2191 - acc: 0.9041\n",
      "Epoch 114/150\n",
      "71632/71632 [==============================] - 44s 610us/sample - loss: 0.2131 - acc: 0.9071\n",
      "Epoch 115/150\n",
      "71632/71632 [==============================] - 44s 616us/sample - loss: 0.2286 - acc: 0.8996\n",
      "Epoch 116/150\n",
      "71632/71632 [==============================] - 48s 669us/sample - loss: 0.2075 - acc: 0.9105\n",
      "Epoch 117/150\n",
      "71632/71632 [==============================] - 52s 730us/sample - loss: 0.2067 - acc: 0.9106\n",
      "Epoch 118/150\n",
      "71632/71632 [==============================] - 49s 687us/sample - loss: 0.1921 - acc: 0.9172\n",
      "Epoch 119/150\n",
      "71632/71632 [==============================] - 47s 653us/sample - loss: 0.1994 - acc: 0.9146\n",
      "Epoch 120/150\n",
      "71632/71632 [==============================] - 44s 611us/sample - loss: 0.1841 - acc: 0.9208\n",
      "Epoch 121/150\n",
      "71632/71632 [==============================] - 55s 765us/sample - loss: 0.1873 - acc: 0.9204\n",
      "Epoch 122/150\n",
      "71632/71632 [==============================] - 51s 718us/sample - loss: 0.1779 - acc: 0.9243\n",
      "Epoch 123/150\n",
      "71632/71632 [==============================] - 49s 684us/sample - loss: 0.1824 - acc: 0.9207\n",
      "Epoch 124/150\n",
      "71632/71632 [==============================] - 40s 554us/sample - loss: 0.1808 - acc: 0.9234\n",
      "Epoch 125/150\n",
      "71632/71632 [==============================] - 54s 751us/sample - loss: 0.1757 - acc: 0.9254\n",
      "Epoch 126/150\n",
      "71632/71632 [==============================] - 42s 585us/sample - loss: 0.1799 - acc: 0.9231\n",
      "Epoch 127/150\n",
      "71632/71632 [==============================] - 50s 703us/sample - loss: 0.1674 - acc: 0.9290\n",
      "Epoch 128/150\n",
      "71632/71632 [==============================] - 45s 632us/sample - loss: 0.1710 - acc: 0.9261\n",
      "Epoch 129/150\n",
      "71632/71632 [==============================] - 52s 719us/sample - loss: 0.1619 - acc: 0.9320\n",
      "Epoch 130/150\n",
      "71632/71632 [==============================] - 49s 689us/sample - loss: 0.1470 - acc: 0.9382\n",
      "Epoch 131/150\n",
      "71632/71632 [==============================] - 48s 664us/sample - loss: 0.1469 - acc: 0.9381\n",
      "Epoch 132/150\n",
      "71632/71632 [==============================] - 43s 604us/sample - loss: 0.1510 - acc: 0.9370\n",
      "Epoch 133/150\n",
      "71632/71632 [==============================] - 58s 808us/sample - loss: 0.1434 - acc: 0.9415\n",
      "Epoch 134/150\n",
      "71632/71632 [==============================] - 52s 729us/sample - loss: 0.1585 - acc: 0.9333\n",
      "Epoch 135/150\n",
      "71632/71632 [==============================] - 48s 667us/sample - loss: 0.1409 - acc: 0.9418\n",
      "Epoch 136/150\n",
      "71632/71632 [==============================] - 41s 576us/sample - loss: 0.1406 - acc: 0.9415\n",
      "Epoch 137/150\n",
      "71632/71632 [==============================] - 52s 730us/sample - loss: 0.1363 - acc: 0.9435\n",
      "Epoch 138/150\n",
      "71632/71632 [==============================] - 52s 730us/sample - loss: 0.1424 - acc: 0.9413\n",
      "Epoch 139/150\n",
      "71632/71632 [==============================] - 52s 730us/sample - loss: 0.1317 - acc: 0.9453\n",
      "Epoch 140/150\n",
      "71632/71632 [==============================] - 41s 578us/sample - loss: 0.1251 - acc: 0.9484\n",
      "Epoch 141/150\n",
      "71632/71632 [==============================] - 54s 750us/sample - loss: 0.1296 - acc: 0.9474\n",
      "Epoch 142/150\n",
      "71632/71632 [==============================] - 44s 607us/sample - loss: 0.1360 - acc: 0.9451\n",
      "Epoch 143/150\n",
      "71632/71632 [==============================] - 44s 611us/sample - loss: 0.1397 - acc: 0.9424\n",
      "Epoch 144/150\n",
      "71632/71632 [==============================] - 52s 729us/sample - loss: 0.1216 - acc: 0.9505\n",
      "Epoch 145/150\n",
      "71632/71632 [==============================] - 45s 622us/sample - loss: 0.1074 - acc: 0.9574\n",
      "Epoch 146/150\n",
      "71632/71632 [==============================] - 45s 633us/sample - loss: 0.1119 - acc: 0.9549\n",
      "Epoch 147/150\n",
      "71632/71632 [==============================] - 56s 786us/sample - loss: 0.1127 - acc: 0.9545\n",
      "Epoch 148/150\n",
      "71632/71632 [==============================] - 50s 699us/sample - loss: 0.1121 - acc: 0.9547\n",
      "Epoch 149/150\n",
      "71632/71632 [==============================] - 50s 703us/sample - loss: 0.1446 - acc: 0.9414\n",
      "Epoch 150/150\n",
      "71632/71632 [==============================] - 50s 694us/sample - loss: 0.0998 - acc: 0.9597\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a91139d550>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential([\n",
    "    embedding_layer,\n",
    "    Bidirectional(CuDNNLSTM(256, return_sequences=False)),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.fit(padded_X_train, y_train, epochs=150, batch_size = 400, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14982, 26770, 26771, ...,     0,     0,     0],\n",
       "       [   60,     2,   563, ...,     0,     0,     0],\n",
       "       [    2,    59,     4, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  111,     3, 47046, ...,     0,     0,     0],\n",
       "       [ 1300,     0,     0, ...,     0,     0,     0],\n",
       "       [   15,     2,   219, ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_X_test = t.texts_to_sequences(X_test)\n",
    "padded_X_test = pad_sequences(encoded_X_test, maxlen=max_sent_len, padding='post')\n",
    "padded_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17909, 74)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_test = model4.predict_classes(padded_X_test)\n",
    "pred_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17909,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4f30429c224a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = Saver(max_to_keep=1) \n",
    "with tf.Session() as sess:\n",
    "    # train your model, then:\n",
    "    savePath = saver.save(sess, 'model/trained_model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
